{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00cc3e0-1e36-4ea3-90bd-ee8fa801bb45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.geocoders import Photon\n",
    "from IPython import get_ipython\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from iso3166 import countries_by_numeric\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "### READ DATASETS ###\n",
    "\n",
    "df_soep = pd.read_parquet(\"soep2.gzip\")\n",
    "df_ivs = pd.read_stata(\"ivs_edited.dta\", convert_categoricals=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc584d2-fa1a-4af1-8128-ecacaca247f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### IVS ###\n",
    "\n",
    "# Country names are received from an API and saved as cache\n",
    "\n",
    "CACHE_FILENAME = \"country_data_cache.json\"\n",
    "\n",
    "if os.path.exists(CACHE_FILENAME):\n",
    "    with open(CACHE_FILENAME, \"r\") as file:\n",
    "        cache = json.load(file)\n",
    "else:\n",
    "    cache = {\"numeric_to_alpha3\": {}, \"country_details\": {}}\n",
    "\n",
    "def numeric_to_alpha3(numeric_codes):\n",
    "    str_codes = numeric_codes.astype(str)\n",
    "    return str_codes.map(cache[\"numeric_to_alpha3\"])\n",
    "\n",
    "df_ivs[\"alpha3_codenum\"] = numeric_to_alpha3(df_ivs[\"c_codenum\"])\n",
    "\n",
    "def get_country_details(alpha3_codes):\n",
    "    def extract_detail(detail, x):\n",
    "        try:\n",
    "            return x.get(detail) if x is not None and isinstance(x, dict) else None\n",
    "        except AttributeError:\n",
    "            return None\n",
    "    \n",
    "    country_details = alpha3_codes.map(cache[\"country_details\"])\n",
    "    country_names = country_details.apply(lambda x: extract_detail(\"country_name\", x))\n",
    "    country_abbrs = country_details.apply(lambda x: extract_detail(\"country_abbr\", x))\n",
    "\n",
    "    return country_names, country_abbrs\n",
    "\n",
    "df_ivs[\"country_name\"], df_ivs[\"country_abbr\"] = get_country_details(df_ivs[\"alpha3_codenum\"])\n",
    "\n",
    "\n",
    "# Save the cache back to the file\n",
    "with open(CACHE_FILENAME, \"w\") as file:\n",
    "    json.dump(cache, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe5eb22-ddc8-4157-b481-adc3f0acb0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop original column\"\n",
    "df_ivs.drop(columns=[\"c_codenum\"])\n",
    "\n",
    "# Create gender dummy variable\"\n",
    "df_ivs.sex.replace({1: \"0\", 2: \"1\"}, inplace=True)\n",
    "\n",
    "# Create cohort variable\"\n",
    "\n",
    "bins = [18, 30, 45, 60, np.inf]\n",
    "names = [\"18-29\", \"30-44\", \"45-59\", \"60+\"]\n",
    "\n",
    "df_ivs[\"age_cohort\"] = pd.cut(df_ivs[\"age\"], bins, labels=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e352e-008a-4286-ad1f-b56664e87735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Weighting function using the IVS weights\n",
    "def weighted_mean(df, val, weight):\n",
    "    if df.empty or df[weight].isnull().all():\n",
    "        return np.nan\n",
    "    return (df[val] * df[weight]).sum() / df[weight].sum()\n",
    "\n",
    "# Determine all unique values for countries, age cohorts, and years\n",
    "unique_countries = df_ivs['alpha3_codenum'].unique()\n",
    "unique_ages = df_ivs['age_cohort'].unique()\n",
    "unique_years = range(df_ivs['year_survey'].min(), df_ivs['year_survey'].max() + 1)\n",
    "\n",
    "# Create a DataFrame with all possible combinations of these values\n",
    "all_combinations = pd.DataFrame([(country, age, year) for country in unique_countries for age in unique_ages for year in unique_years], \n",
    "                                columns=['alpha3_codenum', 'age_cohort', 'year_survey'])\n",
    "\n",
    "# Merge with the original df_ivs dataset\n",
    "merged_df = all_combinations.merge(df_ivs, on=['alpha3_codenum', 'age_cohort', 'year_survey'], how='left')\n",
    "\n",
    "# Calculate the weighted mean for life satisfaction\n",
    "def process_group(group):\n",
    "    wm = weighted_mean(group, 'lifesatisfaction', 'weight')\n",
    "    group['lifesatisfaction'] = wm\n",
    "    return group\n",
    "\n",
    "# Merge life weighted mean life satisfaction data\n",
    "df_processed = merged_df.groupby(['alpha3_codenum', 'age_cohort', 'year_survey']).apply(process_group).reset_index(drop=True)\n",
    "\n",
    "# Save country-year-cohort life satisfaction data in JSON file\n",
    "json_data = {}\n",
    "\n",
    "for _, row in df_processed.iterrows():\n",
    "    country_code = row['alpha3_codenum']\n",
    "    cohort = row['age_cohort']\n",
    "    year = row['year_survey']\n",
    "    lifesatisfaction = row['lifesatisfaction']\n",
    "    \n",
    "    if country_code not in json_data:\n",
    "        json_data[country_code] = {}\n",
    "    if cohort not in json_data[country_code]:\n",
    "        json_data[country_code][cohort] = {}\n",
    "    json_data[country_code][cohort][year] = lifesatisfaction\n",
    "\n",
    "with open(\"lifesatisfaction_data.json\", \"w\") as outfile:\n",
    "    json.dump(json_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449076e-6c20-4fe4-8b39-cd9ed3319e8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### SOEP ###\n",
    "\n",
    "# Keep only rows with valid life satisfaction data\n",
    "df_soep = df_soep[df_soep['satisfaction'].ge(0)]\n",
    "\n",
    "# Calculate age(²) and map cohorts\n",
    "df_soep[\"age\"] = df_soep['syear'] - df_soep['gebjahr']\n",
    "\n",
    "df_soep['age_sq'] = (df_soep['age'] ** 2)\n",
    "\n",
    "df_soep[\"cohort\"] = pd.cut(df_soep[\"age\"], bins=[18, 30, 45, 60, np.inf], labels=[\"18-29\", \"30-44\", \"45-59\", \"60+\"])\n",
    "\n",
    "dummies = pd.get_dummies(df_soep['cohort'], prefix='cohort')\n",
    "\n",
    "# Rename dummy columns for later use in Stata\n",
    "new_dummy_columns = {col: col.replace('-', '_').replace('+', '') for col in dummies.columns}\n",
    "dummies.rename(columns=new_dummy_columns, inplace=True)\n",
    "\n",
    "# Add dummy series to SOEP dataframe\n",
    "df_soep = pd.concat([df_soep, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5559e97-3394-4e8d-a0dd-d2495f168171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform weird Stata country labels\n",
    "df_soep[\"corigin_str\"] = df_soep[\"corigin_str\"].astype(str).apply(lambda x: \" \".join(x.split(\" \")[1:]))\n",
    "\n",
    "# Select only valid satisfaction values, only valid countries\n",
    "df_soep = df_soep[df_soep['satisfaction'].ge(0) & ~df_soep[\"corigin_str\"].isin([\"No Answer\", \"Kosovo-Albania\"])]\n",
    "\n",
    "# Remove invalid countries (redundant?)\n",
    "exclude_values = [\"No Answer\", \"Kosovo-Albania\"]\n",
    "df_soep = df_soep[~df_soep[\"corigin_str\"].isin(exclude_values)]\n",
    "\n",
    "# Logarithm applied to income variable\n",
    "df_soep['gen_income'] = df_soep['gen_income'].replace(-2, 1)\n",
    "df_soep['gen_income'] = df_soep['gen_income'].replace(0, 1)\n",
    "df_soep['gen_income'] = df_soep['gen_income'].apply(lambda x: x if x > 0 else None)\n",
    "\n",
    "df_soep['gen_income'] = np.log(df_soep['gen_income'])\n",
    "\n",
    "# Look up ISO country codes and save them in JSON for future use\n",
    "\n",
    "def convert_to_alpha3(country_name):\n",
    "    try:\n",
    "        return pycountry.countries.get(name=country_name).alpha_3\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            return pycountry.countries.search_fuzzy(country_name)[0].alpha_3\n",
    "        except LookupError:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "if os.path.exists(\"country_mapping.json\"):\n",
    "    with open(\"country_mapping.json\", \"r\") as file:\n",
    "        country_to_alpha3 = json.load(file)\n",
    "else:\n",
    "    country_to_alpha3 = {}\n",
    "\n",
    "unique_countries = df_soep[\"corigin_str\"].unique()\n",
    "for country in unique_countries:\n",
    "    if country not in country_to_alpha3:\n",
    "        country_to_alpha3[country] = convert_to_alpha3(country)\n",
    "\n",
    "with open(\"country_mapping.json\", \"w\") as file:\n",
    "    json.dump(country_to_alpha3, file)\n",
    "\n",
    "def map_origin_str_to_iso(row, mapping_dict):\n",
    "    if row[\"corigin_str\"] in mapping_dict:\n",
    "        return mapping_dict[row[\"corigin_str\"]]\n",
    "    else:\n",
    "        return row[\"corigin_iso\"]\n",
    "\n",
    "# Manually map unrecognized country codes\n",
    "\n",
    "mapping_dict = {\n",
    "    \"Ex-Yugoslavia\": \"YUG\",\n",
    "    \"Bosnia-Herzegovina\": \"BIH\",\n",
    "    \"Eastern Europe\": \"?EE\",\n",
    "    \"Columbia\": \"COL\",\n",
    "    \"Moldavia\": \"MDA\",\n",
    "    \"Kosovo\": \"XXK\",\n",
    "    \"UAE\": \"ARE\",\n",
    "    \"Benelux\": \"?BL\",\n",
    "    \"Ivory Coast\": \"CIV\",\n",
    "    \"No Nationality\": \"?NN\",\n",
    "    \"Trinidad-Tobago\": \"TTO\",\n",
    "}\n",
    "\n",
    "df_soep[\"corigin_iso\"] = df_soep[\"corigin_str\"].map(country_to_alpha3)\n",
    "df_soep[\"corigin_iso\"] = df_soep.apply(lambda row: mapping_dict.get(row[\"corigin_str\"], row[\"corigin_iso\"]), axis=1)\n",
    "\n",
    "\n",
    "# Manually map wrong country names\n",
    "df_soep[\"corigin_str\"] = df_soep.corigin_str.replace(\"Columbia\", \"Colombia\");\n",
    "df_soep[\"corigin_str\"] = df_soep.corigin_str.replace(\"Moldavia\", \"Moldova\");\n",
    "\n",
    "\n",
    "# Calculate years in Germany\"\n",
    "\n",
    "df_soep[\"y_in_germany\"] = np.where(df_soep[\"immiyear\"] >= 1, df_soep[\"syear\"] - df_soep[\"immiyear\"], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5be694-bf5c-44f0-af2a-0da74f3f9354",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "oecd = pd.read_csv(\"thesis_oecd.csv\")\n",
    "\n",
    "# Adjust cohort mapping\n",
    "age_cohort_mapping = {\n",
    "    \"18-29\": \"Young\",\n",
    "    \"30-44\": \"Middle-aged\",\n",
    "    \"45-59\": \"Middle-aged\",\n",
    "    \"60+\": \"Old\"\n",
    "}\n",
    "\n",
    "oecd['Age'] = oecd['Age'].map(age_cohort_mapping)\n",
    "\n",
    "# Rename columns to the ones previously used\n",
    "oecd = oecd.rename(columns={\n",
    "    'LOCATION': 'alpha3_codenum',\n",
    "    'Age': 'age_cohort',\n",
    "    'Time': 'year_survey',\n",
    "    'Value': 'lifesatisfaction'\n",
    "})\n",
    "\n",
    "# Save in JSON\n",
    "json_data = {}\n",
    "for _, row in oecd.iterrows():\n",
    "    country_code = row['alpha3_codenum']\n",
    "    cohort = row['age_cohort']\n",
    "    year = row['year_survey']\n",
    "    lifesatisfaction = row['lifesatisfaction']\n",
    "\n",
    "    if country_code not in json_data:\n",
    "        json_data[country_code] = {}\n",
    "    if cohort not in json_data[country_code]:\n",
    "        json_data[country_code][cohort] = {}\n",
    "    json_data[country_code][cohort][year] = lifesatisfaction\n",
    "\n",
    "with open(\"lifesatisfaction_data_oecd.json\", \"w\") as outfile:\n",
    "    json.dump(json_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61853450-6e14-4a06-9c63-32c34956ad74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"lifesatisfaction_data.json\", \"r\") as infile:\n",
    "    lifesatisfaction_data = json.load(infile)\n",
    "with open(\"lifesatisfaction_data_oecd.json\", \"r\") as infile:\n",
    "    oecd_data = json.load(infile)\n",
    "\n",
    "# Merging the data\n",
    "for country, cohorts in oecd_data.items():\n",
    "    if country not in lifesatisfaction_data:\n",
    "        lifesatisfaction_data[country] = {}\n",
    "        \n",
    "    for cohort, years in cohorts.items():\n",
    "        if cohort not in lifesatisfaction_data[country]:\n",
    "            lifesatisfaction_data[country][cohort] = {}\n",
    "            \n",
    "        for year, value in years.items():\n",
    "            lifesatisfaction_data[country][cohort][year] = value\n",
    "\n",
    "# Save merged data\n",
    "with open(\"merged_lifesatisfaction_data.json\", \"w\") as outfile:\n",
    "    json.dump(lifesatisfaction_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4adc05c-c8b1-4cc8-a4b0-4e92cb66c8cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load life satisfaction data\n",
    "with open(\"merged_lifesatisfaction_data.json\", \"r\") as file:\n",
    "    df = json.load(file)\n",
    "\n",
    "# Convert JSON to DataFrame\n",
    "df_list = []\n",
    "for country, cohorts in df.items():\n",
    "    for cohort, years in cohorts.items():\n",
    "        for year, value in years.items():\n",
    "            df_list.append({'_key': country, 'Cohort/Year': f\"{cohort}/{year}\", 'Lifesatisfaction': value})\n",
    "df = pd.DataFrame(df_list)\n",
    "\n",
    "# Get all unique cohorts (excluding 'NaN')\n",
    "all_cohorts = ['18-29', '30-44', '45-59', '60+']\n",
    "\n",
    "# Identify countries with 'NaN' cohorts and replicate data for all cohorts\n",
    "nan_cohort_countries = df[df['Cohort/Year'].str.startswith('NaN/')]['_key'].unique()\n",
    "replicated_rows = []\n",
    "for country in nan_cohort_countries:\n",
    "    country_data = df[(df['_key'] == country) & (df['Cohort/Year'].str.startswith('NaN/'))].copy()\n",
    "    for cohort in all_cohorts:\n",
    "        new_data = country_data.copy()\n",
    "        new_data['Cohort/Year'] = new_data['Cohort/Year'].str.replace('NaN', cohort)\n",
    "        replicated_rows.append(new_data)\n",
    "\n",
    "df = pd.concat([df] + replicated_rows, ignore_index=True)\n",
    "\n",
    "# Remove rows with 'NaN' cohorts\n",
    "df = df[~df['Cohort/Year'].str.startswith('NaN/')]\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivot = df.pivot_table(index='_key', columns='Cohort/Year', values='Lifesatisfaction', aggfunc='first')\n",
    "\n",
    "# Apply Linear interpolation combined with LOCF/NOCB\n",
    "df_interpolated = df_pivot.interpolate(method='linear', axis=1, limit_direction='both')\n",
    "\n",
    "missing_oecd_countries = [country for country in oecd_countries if country not in df_interpolated.index]\n",
    "if missing_oecd_countries:\n",
    "    print(\"These OECD countries are missing from the df_interpolated dataframe:\", missing_oecd_countries)\n",
    "\n",
    "# Apply extrapolation only for OECD countries\n",
    "for country in oecd_countries:\n",
    "    df_interpolated.loc[country] = df_interpolated.loc[country].bfill().ffill()\n",
    "\n",
    "df_melted = pd.melt(df_interpolated.reset_index(), id_vars=['_key'], value_name='Lifesatisfaction')\n",
    "\n",
    "# Split Cohort and Year from the melted dataframe\n",
    "df_melted[['Cohort', 'Year']] = df_melted['Cohort/Year'].str.split('/', n=1, expand=True)\n",
    "\n",
    "filled_data = {}\n",
    "for _, row in df_melted.iterrows():\n",
    "    country = row['_key']\n",
    "    cohort = row['Cohort']\n",
    "    year = row['Year']\n",
    "    value = row['Lifesatisfaction']\n",
    "\n",
    "    if country not in filled_data:\n",
    "        filled_data[country] = {}\n",
    "    if cohort not in filled_data[country]:\n",
    "        filled_data[country][cohort] = {}\n",
    "    filled_data[country][cohort][year] = value\n",
    "\n",
    "# Save the filled data as a JSON\n",
    "with open(\"lifesatisfaction_data_filled.json\", \"w\") as outfile:\n",
    "    json.dump(filled_data, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c395b33-4b9f-44f8-a756-788adc353976",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rename columns to prepare merge\n",
    "df_ivs[\"alpha3_codenum\"] = df_ivs[\"alpha3_codenum\"].astype(str)\n",
    "df_ivs = df_ivs.reset_index()  # reset index if your columns are currently indices\n",
    "df_ivs = df_ivs.rename(\n",
    "    columns={\n",
    "        \"alpha3_codenum\": \"corigin_iso\",\n",
    "        \"year_survey\": \"syear\",\n",
    "        \"age_cohort\": \"cohort\",\n",
    "        \"lifesatisfaction\": \"satisf_peers\",\n",
    "    }\n",
    ")\n",
    "df_ivs.set_index([\"corigin_iso\", \"cohort\", \"syear\"], inplace=True)\n",
    "\n",
    "# Drop missing data\"\n",
    "\n",
    "df_ivs.dropna(subset=[\"index\", \"satisf_peers\"], inplace=True)\n",
    "df_ivs.reset_index(inplace=True)\n",
    "df_ivs = df_ivs[[\"corigin_iso\", \"syear\", \"cohort\", \"sex\", \"satisf_peers\", \"age\", \"subj_health\", \"educ_recoded\", \"educ_isced97\", \"educ_isced11\", \"educ_casmin\", \"occ_status_siops\", \"occ_status_isei\", \"occ_status_epg11\", \"empl_status\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dab8c-a026-4cb9-99bd-001dd74272f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove dumplicate that sometimes appear when running the same code cell twice\n",
    "def resolve_duplicate_columns(df):\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in cols[cols.duplicated()].unique():\n",
    "        cols[cols[cols == dup].index.values.tolist()] = [dup + '_' + str(i) if i != 0 else dup for i in range(sum(cols == dup))]\n",
    "    df.columns = cols\n",
    "\n",
    "# Applying the function to resolve duplicate column names\n",
    "resolve_duplicate_columns(df_soep)\n",
    "\n",
    "# Save temporary datasets (to prevent having to run the full code each time)\n",
    "df_soep.to_parquet(\"soep_unmerged.gzip\", compression='gzip')\n",
    "df_ivs.to_parquet(\"ivs_unmerged.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c986f-1443-40d8-9236-cba0fd8dc240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load temporary datasets\n",
    "df_soep = pd.read_parquet(\"soep_unmerged.gzip\")\n",
    "df_ivs = pd.read_parquet(\"ivs_unmerged.gzip\")\n",
    "\n",
    "\n",
    "# Merge IVS and OECD data from JSON to SOEP\n",
    "\n",
    "with open(\"lifesatisfaction_data_filled.json\", \"r\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "df_list = []\n",
    "for country, cohorts in json_data.items():\n",
    "    for cohort, years in cohorts.items():\n",
    "        for year, value in years.items():\n",
    "            df_list.append({\n",
    "                'corigin_iso': country,\n",
    "                'cohort': cohort,\n",
    "                'syear': int(year),\n",
    "                'satisf_peers': value\n",
    "            })\n",
    "\n",
    "df_json = pd.DataFrame(df_list)\n",
    "\n",
    "df_soep = df_soep.merge(df_json, on=['corigin_iso', 'syear', 'cohort'], how='left')\n",
    "\n",
    "# Create satisfaction to peers difference variable\n",
    "df_soep[\"satisf_diff\"] = df_soep[\"satisfaction\"] - df_soep[\"satisf_peers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad63b02-ccd4-41ee-9181-b1d359cf6b4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create religion dummies\n",
    "df_soep['rel_christ'] = np.where(df_soep['religion'].isin([1, 2, 3, 7]), 1, 0)\n",
    "df_soep['rel_musl'] = np.where(df_soep['religion'].isin([4, 8, 9, 10]), 1, 0)\n",
    "df_soep['rel_unaffiliated'] = np.where(df_soep['religion'] == 6, 1, 0)\n",
    "df_soep['rel_other'] = np.where(df_soep['religion'] == 5, 1, 0)\n",
    "df_soep['rel_unknown'] = np.where(~df_soep['religion'].isin([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea7d474-9004-43f9-ab8b-053fdd3c8f46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geolocator = Photon(user_agent=\"myGeocoder\")\n",
    "\n",
    "# Load existing coordinate cache, if available\n",
    "try:\n",
    "    with open(\"country_coords.json\", \"r\") as f:\n",
    "        coordinate_cache = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    coordinate_cache = {}\n",
    "\n",
    "def get_coordinates(country):\n",
    "    location = geolocator.geocode(country)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    return None, None  # Return tuple of None values for missing coordinates\n",
    "\n",
    "# Get coordinates for all countries\n",
    "unique_countries = df_soep[\"corigin_str\"].unique()\n",
    "coords_dict = {country: get_coordinates(country) for country in unique_countries}\n",
    "\n",
    "# Update cache\n",
    "coordinate_cache.update(coords_dict)\n",
    "with open(\"country_coords.json\", \"w\") as f:\n",
    "    json.dump(coordinate_cache, f)\n",
    "\n",
    "# Map coordinates to countries\n",
    "df_soep['country_coords'] = df_soep['corigin_str'].map(coordinate_cache)\n",
    "\n",
    "# Ensure that every value in df_soep['country_coords'] is a tuple\n",
    "df_soep['country_coords'] = df_soep['country_coords'].apply(\n",
    "    lambda x: x if isinstance(x, tuple) else (None, None)\n",
    ")\n",
    "\n",
    "# Calculate distance to Germany\n",
    "germany_coords = coordinate_cache.get(\"Germany\", (None, None))\n",
    "if germany_coords == (None, None):\n",
    "    germany_coords = get_coordinates(\"Germany\")\n",
    "    coordinate_cache[\"Germany\"] = germany_coords\n",
    "    with open(\"country_coords.json\", \"w\") as f:\n",
    "        json.dump(coordinate_cache, f)\n",
    "\n",
    "# Calculate the distance using the updated column\n",
    "df_soep['dist_origin'] = df_soep['country_coords'].apply(\n",
    "    lambda x: geodesic(germany_coords, x).kilometers if x[0] is not None and x[1] is not None else None\n",
    ")\n",
    "\n",
    "# Drop the temporary column\n",
    "df_soep.drop('country_coords', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9c8412-6f30-4645-b73f-385fda49ef93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transforming variables\n",
    "\n",
    "df_soep['bad_health'] = df_soep['curr_health'].apply(lambda x: 1 if x in [4, 5] else (0 if x in [1, 2, 3] else np.nan))\n",
    "\n",
    "df_soep['lang_profic'] = df_soep.apply(lambda row: \n",
    "                                      1 if row['lang_oral'] in [1, 2] or \n",
    "                                         row['lang_usl'] in [1, 3] or \n",
    "                                         row['ger_newspaper'] in [3, 4, 5] or \n",
    "                                         row['lang_fam'] in [1, 4] or \n",
    "                                         row['lang_friends'] == 1 or \n",
    "                                         row['ger_premig'] in [1, 2] or \n",
    "                                         row['lang_oral2'] in [1, 2] \n",
    "                                      else (0 if row['lang_oral'] in [3, 4, 5] else np.nan), \n",
    "                                      axis=1)\n",
    "df_soep['lang_profic'] = df_soep.groupby('pid')['lang_profic'].ffill()\n",
    "\n",
    "df_soep['edu_collg'] = df_soep['edu_collg'].apply(lambda x: 1 if x > 0 else (0 if x == 0 else np.nan))\n",
    "\n",
    "df_soep['edu_novocat'] = df_soep['edu_novocat'].apply(lambda x: 1 if x == 1 else (0 if x in [2, 3] else np.nan))\n",
    "\n",
    "df_soep['gen_wpartner'] = df_soep['gen_family'].apply(lambda x: 1 if x in [1, 7] else (0 if x >= 0 else np.nan))\n",
    "\n",
    "df_soep['gen_seppart'] = df_soep['gen_family'].apply(lambda x: 1 if x in [2, 6, 7] else (0 if x >= 0 else np.nan))\n",
    "\n",
    "df_soep['gen_wid_div'] = np.where(df_soep['gen_family'].isin([4, 5]), 1, \n",
    "                                  np.where(df_soep['gen_family'] < 0, np.nan, 0))\n",
    "\n",
    "df_soep['gen_single'] = df_soep['gen_family'].apply(lambda x: 1 if x == 3 else (0 if x >= 0 else np.nan))\n",
    "\n",
    "df_soep['gen_notrain'] = df_soep['gen_edu97'].apply(lambda x: 1 if x in [0, 1, 2] else (0 if x >= 0 else np.nan))\n",
    "\n",
    "df_soep['gen_tertiary'] = df_soep['gen_edu97'].apply(lambda x: 1 if x in [5, 6] else (0 if x >= 0 else np.nan))\n",
    "\n",
    "df_soep['gen_employed'] = df_soep['gen_emplmnt'].apply(lambda x: 1 if x in [1, 2] else (0 if x >= 3 else np.nan))\n",
    "\n",
    "df_soep['gen_vocation'] = df_soep['gen_emplmnt'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "df_soep['gen_retired'] = df_soep['gen_typeunempl'].apply(lambda x: 1 if x == 2 else (0 if x > 2 else np.nan))\n",
    "df_soep['gen_retired'] = df_soep.groupby('pid')['gen_retired'].ffill()\n",
    "\n",
    "df_soep['gen_regunempl'] = df_soep['gen_typeunempl'].apply(lambda x: 1 if x == 6 else (0 if x != 6 and x > 0 else np.nan))\n",
    "\n",
    "df_soep['refugee'] = df_soep['ig_grp'].apply(lambda x: 1 if x == 5 else (np.nan if x == -5 else 0))\n",
    "\n",
    "df_soep['ig_cont_fam'] = df_soep['ig_cont_fam'].apply(lambda x: 1 if x == 1 else (0 if x == 2 else np.nan))\n",
    "\n",
    "df_soep['feel_german'] = df_soep['feel_german'].apply(lambda x: 1 if x in [1, 2] else (0 if x in [3, 4, 5] else np.nan))\n",
    "\n",
    "df_soep['visit_germ'] = df_soep['visit_germ'].apply(lambda x: 1 if x == 1 else (0 if x == 2 else np.nan))\n",
    "\n",
    "df_soep.loc[df_soep['edu_years'] < 0, 'edu_years'] = np.nan\n",
    "\n",
    "df_soep = df_soep.sort_values(by=['pid', 'syear'])\n",
    "\n",
    "# Forwards fill variables\n",
    "df_soep['feel_german'] = df_soep.groupby('pid')['feel_german'].ffill()\n",
    "df_soep['visit_germ'] = df_soep.groupby('pid')['visit_germ'].ffill()\n",
    "\n",
    "df_soep['want_stay'] = df_soep.apply(lambda row: 1 if row['stay_ger'] == 1 or row['stay_ger13'] == 1 else (0 if row['stay_ger'] == 2 or row['stay_ger13'] == 2 else np.nan), axis=1)\n",
    "\n",
    "df_soep['german'] = df_soep['gen_nation'].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "columns_to_check = [\n",
    "    \"sat_work\", \"sat_hhinc\", \"sat_dwell\", \n",
    "    \"sat_leisure\", \"sat_family\", \"sat_sleep\"\n",
    "]\n",
    "\n",
    "for column in columns_to_check:\n",
    "    df_soep.loc[~df_soep[column].between(0, 10), column] = np.nan\n",
    "\n",
    "df_soep['discrimination'] = df_soep.apply(\n",
    "    lambda row: 1 if any(x in [1, 2] for x in [row['disadv_origin1'], row['disadv_origin2']]) \n",
    "                else (0 if any(x in [3, -2, -1] for x in [row['disadv_origin1'], row['disadv_origin2']]) \n",
    "                      else np.nan), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_soep['discrimination'] = df_soep.groupby('pid')['discrimination'].ffill()\n",
    "\n",
    "df_soep['dist_city'] = df_soep['dist_city'].apply(lambda x: 1 if x in [1, 2] else 0)\n",
    "df_soep['dist_city'] = df_soep.groupby('hid')['dist_city'].ffill()\n",
    "\n",
    "def assign_social(row):\n",
    "    if (row['lonely'] in [3, 4] or \n",
    "        row['nbh_relation'] in [1, 2, 3] or \n",
    "        row['nbh_freq'] in [1, 2] or \n",
    "        row['cl_friends'] > 2):\n",
    "        return 1\n",
    "    elif (row['lonely'] in [1, 2] or \n",
    "          row['cl_friends'] in [0, 1]):\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "df_soep['social'] = df_soep.apply(assign_social, axis=1)\n",
    "\n",
    "df_soep['ig_cont_fam'] = df_soep['ig_cont_fam'].apply(lambda x: 1 if x == 1 else (0 if x in [0, -2] else np.nan))\n",
    "\n",
    "df_soep['xenophobia'] = df_soep['xenophobia'].apply(lambda x: 1 if x in [1, 2] else (0 if x in [3, -1, -2] else np.nan))\n",
    "\n",
    "df_natives = df_soep[df_soep['migback'] == 1]\n",
    "\n",
    "def weighted_satisfaction(group):\n",
    "    return sum(group['satisfaction'] * group['hochrechnungsfaktor']) / sum(group['hochrechnungsfaktor'])\n",
    "\n",
    "result = df_natives.groupby(['syear', 'cohort']).apply(weighted_satisfaction).reset_index(name='satisf_natives')\n",
    "\n",
    "df_soep = pd.merge(df_soep, result, on=['syear', 'cohort'], how='left')\n",
    "\n",
    "df_soep['satisf_ndiff'] = df_soep['satisfaction'] - df_soep['satisf_natives']\n",
    "\n",
    "df_soep['yig_sq'] = df_soep['y_in_germany'] ** 2\n",
    "df_soep['edu_sq'] = df_soep['edu_years'] ** 2\n",
    "\n",
    "df_soep['num_children'] = df_soep['num_children'].apply(lambda x: 0 if x in [-1, -2] else (x if x >= 0 else np.nan))\n",
    "\n",
    "columns_to_fill = ['social', 'feel_german', 'discrimination', 'xenophobia']\n",
    "\n",
    "# Forwards fill with mode\n",
    "df_soep[columns_to_fill] = df_soep[columns_to_fill].astype('float32')\n",
    "\n",
    "def compute_mode(series):\n",
    "    mode_series = series.mode()\n",
    "    if len(mode_series) == 0:\n",
    "        return np.nan \n",
    "    else:\n",
    "        return mode_series.iloc[0]\n",
    "\n",
    "for col in columns_to_fill:\n",
    "    mode_by_pid = df_soep.groupby('pid')[col].transform(compute_mode)\n",
    "    df_soep[col].fillna(mode_by_pid, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f058e28-6456-435d-a5ec-7d9aade6da86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Right-wing activity in federal state, merge into SOEP, log participants number\n",
    "\n",
    "rwe = pd.read_parquet(\"rwe.gzip\")\n",
    "\n",
    "mapping = {\n",
    "    1: \"Schleswig-Holstein\",\n",
    "    2: \"Hamburg\",\n",
    "    3: \"Niedersachsen\",\n",
    "    4: \"Bremen\",\n",
    "    5: \"Nordrhein-Westfalen\",\n",
    "    6: \"Hessen\",\n",
    "    7: \"Rheinland-Pfalz,Saarland\",\n",
    "    8: \"Baden-Württemberg\",\n",
    "    9: \"Bayern\",\n",
    "    10: \"Saarland\",\n",
    "    11: \"Berlin\",\n",
    "    12: \"Brandenburg\",\n",
    "    13: \"Mecklenburg-Vorpommern\",\n",
    "    14: \"Sachsen\",\n",
    "    15: \"Sachsen-Anhalt\",\n",
    "    16: \"Thüringen\",\n",
    "    -1: \"Unknown\", \n",
    "}\n",
    "\n",
    "df_soep[\"state\"].fillna(-1, inplace=True)\n",
    "df_soep[\"state\"] = df_soep[\"state\"].map(mapping)\n",
    "rwe[\"state\"] = rwe[\"state\"].astype(str)\n",
    "df_soep = pd.merge(df_soep, rwe, on=[\"syear\", \"state\"], how=\"outer\")\n",
    "df_soep['logrwe'] = np.where(df_soep['total_participants'] > 0, np.log(df_soep['total_participants']), np.nan)\n",
    "df_soep.loc[(df_soep['syear'] >= 2005) & (df_soep['syear'] <= 2020) & (df_soep['logrwe'].isna()), 'logrwe'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49753321-2968-4f30-a874-ee24ca419e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variable transformation, select reasonable birth years\n",
    "\n",
    "df_soep[\"sex\"] = df_soep[\"sex\"] - 1\n",
    "\n",
    "df_soep.loc[df_soep['cl_friends'] < 0, 'cl_friends'] = np.nan\n",
    "df_soep.loc[df_soep['lonely'] < 0, 'lonely'] = np.nan\n",
    "df_soep.loc[df_soep['sex'] < 0, 'sex'] = np.nan\n",
    "\n",
    "\n",
    "df_soep = df_soep.loc[df_soep[\"gebjahr\"].ge(1900)]\n",
    "\n",
    "# East/west dummy\n",
    "df_soep[\"eastwest\"] = df_soep[\"eastwest\"] - 1\n",
    "df_soep.loc[df_soep[\"eastwest\"] < 0, \"eastwest\"] = np.nan\n",
    "\n",
    "# Calculate age at migration\"\n",
    "df_soep['migr_age'] = df_soep.apply(lambda row: row['immiyear'] - row['gebjahr'] if row['immiyear'] > 1900 else np.nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ca0fb-5345-42af-8084-24c60bf63f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create continent dummies with help of alpha-3 codes\n",
    "\n",
    "CACHE_FILE = \"country_continent_cache.json\"\n",
    "\n",
    "# Load cache if exists\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        cache = json.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "def alpha3_to_alpha2(alpha_3):\n",
    "    \"\"\"Converts ISO 3166-1 alpha-3 to ISO 3166-1 alpha-2.\"\"\"\n",
    "    return pycountry.countries.get(alpha_3=alpha_3).alpha_2\n",
    "\n",
    "def country_to_continent(country_code):\n",
    "    if country_code in cache:\n",
    "        return cache[country_code]\n",
    "    \n",
    "    try:\n",
    "        alpha_2 = alpha3_to_alpha2(country_code)\n",
    "        continent_code = country_alpha2_to_continent_code(alpha_2)\n",
    "        continent_name = convert_continent_code_to_continent_name(continent_code)\n",
    "        cache[country_code] = continent_name\n",
    "        return continent_name\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "unique_countries_not_in_cache = df_soep.loc[~df_soep[\"corigin_iso\"].isin(cache), \"corigin_iso\"].unique()\n",
    "continent_mapping = {country: country_to_continent(country) for country in unique_countries_not_in_cache}\n",
    "\n",
    "df_soep[\"continent\"] = df_soep[\"corigin_iso\"].map(lambda x: cache.get(x, continent_mapping.get(x)))\n",
    "\n",
    "# Update the cache file\n",
    "with open(CACHE_FILE, \"w\") as f:\n",
    "    json.dump(cache, f)\n",
    "\n",
    "# Add dummies to SOEP\n",
    "continent_dummies = pd.get_dummies(df_soep[\"continent\"], prefix=\"continent\").astype(int)\n",
    "df_soep = pd.concat([df_soep, continent_dummies], axis=1)\n",
    "\n",
    "# Replace spaces for future use\n",
    "df_soep.columns = df_soep.columns.str.replace(\" \", \"_\")\n",
    "\n",
    "continent_dict = {\n",
    "    \"?EE\": \"Europe\",\n",
    "    \"YUG\": \"Europe\", \n",
    "    \"CRI\": \"North America\",  \n",
    "    \"MNG\": \"Asia\",  \n",
    "    \"MOZ\": \"Africa\",  \n",
    "    \"BOL\": \"South America\",  \n",
    "    \"?BL\": \"Europe\",  \n",
    "    \"SLV\": \"North America\",  \n",
    "    \"TCD\": \"Africa\",  \n",
    "    \"?NN\": \"No Nationality\",\n",
    "    \"MUS\": \"Africa\",  \n",
    "    \"TTO\": \"North America\",  \n",
    "    \"CYP\": \"Europe\",  \n",
    "    \"NOR\": \"Europe\",  \n",
    "    \"LBR\": \"Africa\",  \n",
    "    \"DOM\": \"North America\",  \n",
    "    \"SEN\": \"Africa\",  \n",
    "     None: \"Unknown\",\n",
    "    \"HND\": \"North America\",  \n",
    "    \"SGP\": \"Asia\",  \n",
    "    \"ECU\": \"South America\",\n",
    "    \"UGA\": \"Africa\",  \n",
    "    \"NZL\": \"Oceania\",  \n",
    "    \"BWA\": \"Africa\",  \n",
    "    \"MMR\": \"Asia\",  \n",
    "    \"HTI\": \"North America\",  \n",
    "    \"CIV\": \"Africa\",  \n",
    "    \"URY\": \"South America\",  \n",
    "    \"XXK\": \"Europe\",\n",
    "    \"YEM\": \"Asia\",  \n",
    "    \"ZMB\": \"Africa\",  \n",
    "    \"KHM\": \"Asia\",  \n",
    "    \"LSO\": \"Africa\",  \n",
    "    \"MYS\": \"Asia\",  \n",
    "    \"SUR\": \"South America\",  \n",
    "    \"ZWE\": \"Africa\",  \n",
    "    \"NIC\": \"North America\",  \n",
    "    \"MLI\": \"Africa\", \n",
    "    \"RWA\": \"Africa\",  \n",
    "    \"NPL\": \"Asia\",  \n",
    "    \"WSM\": \"Oceania\",  \n",
    "    \"DJI\": \"Africa\",  \n",
    "    \"MDV\": \"Asia\",  \n",
    "    \"MDG\": \"Africa\", \n",
    "    \"BFA\": \"Africa\", \n",
    "    \"HKG\": \"Asia\",  \n",
    "    \"TZA\": \"Africa\",  \n",
    "    \"GUY\": \"South America\",  \n",
    "    \"NER\": \"Africa\",  \n",
    "    \"BEN\": \"Africa\",  \n",
    "    \"QAT\": \"Asia\"   \n",
    "}\n",
    "\n",
    "# Apply manually mapped continents\n",
    "df_soep['continent'] = df_soep['corigin_iso'].map(continent_dict).where(\n",
    "    df_soep['continent'].isna(), \n",
    "    df_soep['continent']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642e68a-fa17-4f8c-9652-80306bee8020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "\n",
    "# Identify OECD countries\n",
    "oecd_countries = oecd['alpha3_codenum'].unique().tolist()\n",
    "\n",
    "df_soep.to_parquet(\"df_soep2.gzip\")\n",
    "oecd_df = df_soep[df_soep['corigin_iso'].isin(oecd_countries)]\n",
    "oecd_df.to_parquet(\"oecd.gzip\")\n",
    "oecd_df.to_csv(\"oecd.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fe1f9c-f7c9-4bef-8bce-f65b1b7c4696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
