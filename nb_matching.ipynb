{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306092a8-ac96-494d-8cb1-a34e51bc8bf2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-02T17:10:50.606878Z",
     "iopub.status.busy": "2023-12-02T17:10:50.606371Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import colormaps as cmaps\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pycountry\n",
    "import seaborn as sns\n",
    "from IPython import get_ipython\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from publib import set_style, fix_style\n",
    "sns.set_style(\"whitegrid\")\n",
    "set_style(['origin'])\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import stata_setup\n",
    "stata_setup.config('/home/jovyan/kay/stata_bin', 'mp', splash=False)\n",
    "from pystata import stata\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", 20)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "df_soep = pd.read_parquet(\"df_soep2.gzip\")\n",
    "df_ivs = pd.read_parquet(\"ivs_unmerged.gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3b670-2989-4044-bbb2-6e23a670e8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create variable to identify source dataset after merging\n",
    "df_ivs[\"soep\"] = 0\n",
    "df_soep[\"soep\"] = 1\n",
    "\n",
    "# Adjust variable names for consistence and merging\n",
    "df_ivs.rename(columns={\"educ_casmin\": \"gen_casmin\", \"subj_health\": \"bad_health\", \"satisf_peers\": \"satisfaction\"}, inplace=True)\n",
    "\n",
    "# Transform variables\n",
    "df_ivs['gen_regunempl'] = np.where(df_ivs['empl_status'] == 7, 1,\n",
    "                                   np.where(df_ivs['empl_status'] < 0, np.nan, 0))\n",
    "\n",
    "df_ivs['bad_health'] = df_ivs['bad_health'].apply(lambda x: 1 if x in [4, 5] else (0 if x in [1, 2, 3] else float('nan')))\n",
    "\n",
    "# Assign IVS individuals unique ids for identification\n",
    "df_ivs[\"pid\"] = 0\n",
    "\n",
    "start_int = int(df_soep['pid'].max()) + 1\n",
    "\n",
    "num_missing = (df_ivs['pid'] == 0).sum()\n",
    "\n",
    "new_pids = range(start_int, start_int + num_missing)\n",
    "\n",
    "df_ivs.loc[df_ivs['pid'] == 0, 'pid'] = list(new_pids)\n",
    "\n",
    "df = pd.concat([df_soep, df_ivs], axis=0)\n",
    "\n",
    "# Drop missing data, use only countries with 50+ pids and max 10000\n",
    "df = df.dropna(subset=[\"age\", \"sex\", \"gen_casmin\", \"bad_health\", \"soep\", \"gen_regunempl\"])\n",
    "\n",
    "valid_countries_df = df.groupby([\"corigin_iso\", \"soep\"])[\"pid\"].nunique().reset_index()\n",
    "\n",
    "valid_countries_df = valid_countries_df[valid_countries_df['pid'] >= 50]\n",
    "\n",
    "valid_countries = valid_countries_df.groupby(\"corigin_iso\").filter(lambda x: len(x) == 2)[\"corigin_iso\"].unique()\n",
    "\n",
    "df_valid_countries = df[df[\"corigin_iso\"].isin(valid_countries)]\n",
    "\n",
    "\n",
    "def sample_pids(x, n=10000):\n",
    "    unique_pids = x[\"pid\"].unique()\n",
    "    sampled_pids = pd.Series(unique_pids).sample(min(len(unique_pids), n), random_state=1).tolist()\n",
    "    return x[x[\"pid\"].isin(sampled_pids)]\n",
    "\n",
    "df_sampled = df_valid_countries.groupby(\"corigin_iso\", group_keys=False).apply(sample_pids)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0df28d11-e7e7-444c-b725-b6dff8173958",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-11-02T01:22:26.863773Z",
     "iopub.status.idle": "2023-11-02T01:22:26.864488Z",
     "shell.execute_reply": "2023-11-02T01:22:26.864280Z",
     "shell.execute_reply.started": "2023-11-02T01:22:26.864256Z"
    },
    "tags": []
   },
   "source": [
    "%%stata -d df_sampled -force\n",
    "\n",
    "* Define treatment based on the SOEP variable\n",
    "gen treatment = soep\n",
    "\n",
    "* Get all countries with observations in both datasets\n",
    "egen count_control = total(soep == 0 & corigin_iso != \"\"), by(corigin_iso)\n",
    "egen count_treatment = total(soep == 1 & corigin_iso != \"\"), by(corigin_iso)\n",
    "gen both_datasets = (count_control > 0 & count_treatment > 0)\n",
    "\n",
    "* Check which countries have data in both datasets\n",
    "tabulate corigin_iso if both_datasets\n",
    "\n",
    "levelsof corigin_iso if both_datasets, local(countries)\n",
    "\n",
    "* Save the full dataset as a temporary file\n",
    "save full_data.dta, replace\n",
    "\n",
    "* Initialize a counter for the temporary files\n",
    "local counter = 1\n",
    "\n",
    "* Loop over each country and apply CEM\n",
    "foreach country in `countries' {\n",
    "\n",
    "    disp \"------------------------------------------------\"\n",
    "    disp \"Performing CEM for country: `country'\"\n",
    "\n",
    "    * Load the full dataset from the temporary file\n",
    "    use full_data.dta, clear\n",
    "\n",
    "    * Define variable to identify pairs\n",
    "    gen pair = (corigin_iso == \"`country'\")\n",
    "        \n",
    "    * Only select from current country\n",
    "    keep if pair == 1\n",
    "    \n",
    "    destring sex, replace force\n",
    "    \n",
    "    * Apply CEM\n",
    "    cem age sex bad_health gen_casmin syear gen_regunempl, treatment(treatment) autocuts(scott) \n",
    "       \n",
    "    * Save the matched data\n",
    "    save matched_data`counter'.dta, replace\n",
    "    \n",
    "    * Increment the counter\n",
    "    local counter = `counter' + 1\n",
    "}\n",
    "\n",
    "* Decrement the counter to match it to the right filenames\n",
    "local counter = `counter' - 1\n",
    "\n",
    "* Load the first matched dataset\n",
    "use matched_data1.dta, clear\n",
    "\n",
    "* Append the rest of the matched datasets\n",
    "forvalues i = 2/`counter' {\n",
    "    append using matched_data`i'.dta\n",
    "}\n",
    "\n",
    "* Identify and drop duplicate rows based on 'syear' and 'pid'\n",
    "duplicates report syear pid\n",
    "duplicates drop syear pid, force\n",
    "\n",
    "* Save the final dataset\n",
    "save cem_weighted.dta, replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc6fc0-2e48-4530-a60e-2f315d807d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load dataset into dataframe\n",
    "cem = pd.read_stata(\"cem_weighted.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0d669-855e-4067-83c7-7d4f202e609e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Determine the columns in cem that are not in df\n",
    "new_cols = [col for col in cem.columns if col not in df.columns]\n",
    "\n",
    "# Merging datasets, only the new columns to prevent duplicates\n",
    "merged_df = df.merge(cem[['pid', 'syear'] + new_cols], on=['pid', 'syear'], how='left')\n",
    "merged_df.corigin_iso.astype('string');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d0b29-9ef6-4272-a3fd-61bc040cc48c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter rows where cem_matched is 1\n",
    "matched_df = merged_df[merged_df['cem_matched'] == 1]\n",
    "\n",
    "# Calculate average values for each covariate for control (soep = 0)\n",
    "control_avg = matched_df[matched_df['soep'] == 0].groupby('corigin_iso').mean().reset_index()\n",
    "\n",
    "# Calculate average values for each covariate for treatment (soep = 1)\n",
    "treatment_avg = matched_df[matched_df['soep'] == 1].groupby('corigin_iso').mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188caf71-ff4b-4782-bacd-9bbee374fb1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the columns to check for imbalance\n",
    "columns_to_check = ['age', 'bad_health', 'gen_casmin', 'syear', 'gen_regunempl']\n",
    "\n",
    "\n",
    "# Function to calculate weighted mean and standard deviation\n",
    "def weighted_stats(df, weight_col, columns):\n",
    "    weighted_means = {}\n",
    "    weighted_stds = {}\n",
    "    for column in columns:\n",
    "        weights = df[weight_col]\n",
    "        values = df[column]\n",
    "        mean = np.average(values, weights=weights)\n",
    "        variance = np.average((values-mean)**2, weights=weights)\n",
    "        std = np.sqrt(variance)\n",
    "        \n",
    "        weighted_means[column] = mean\n",
    "        weighted_stds[column] = std\n",
    "\n",
    "    return weighted_means, weighted_stds\n",
    "\n",
    "# Initializing an empty list to store SMD values rows\n",
    "smd_rows = []\n",
    "matched_df_grouped = matched_df.groupby('corigin_iso')\n",
    "\n",
    "for name, group in matched_df_grouped:\n",
    "    control_group = group[group['soep'] == 0]\n",
    "    treatment_group = group[group['soep'] == 1]\n",
    "    \n",
    "    control_means, control_stds = weighted_stats(control_group, 'cem_weights', columns_to_check)\n",
    "    treatment_means, treatment_stds = weighted_stats(treatment_group, 'cem_weights', columns_to_check)\n",
    "    \n",
    "    # Calculating the SMD\n",
    "    epsilon = 1e-8  # Prevent division by zero\n",
    "    smds = {}\n",
    "\n",
    "    for column in columns_to_check:\n",
    "        mean_diff = abs(treatment_means[column] - control_means[column])  # Taking absolute value\n",
    "        pooled_sd = np.sqrt((control_stds[column]**2 + treatment_stds[column]**2)/2 + epsilon)\n",
    "        smds[f'smd_{column}'] = abs(mean_diff / pooled_sd)  # Taking absolute value\n",
    "\n",
    "    smd_rows.append({'corigin_iso': name, **smds})\n",
    "\n",
    "smd_df = pd.DataFrame(smd_rows)\n",
    "\n",
    "smd_df['average_smd'] = smd_df.iloc[:, 1:].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce50f94a-ddc7-449a-9707-87dc4e072d8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check balance and remove countries with a variable having an imbalance > 0.2\n",
    "def create_love_plot(smd_df, columns_to_check, country):\n",
    "    country_smd = smd_df[smd_df['corigin_iso'] == country]\n",
    "   \n",
    "    smd_columns = [f'smd_{col}' for col in columns_to_check]\n",
    "    \n",
    "    #plt.figure(figsize=(10, 5))\n",
    "    #plt.barh(columns_to_check, country_smd[smd_columns].values.flatten())\n",
    "    \n",
    "    #plt.xlabel('Standardized Mean Differences (SMDs)')\n",
    "    #plt.title(f'Love Plot for {country}')\n",
    "    #plt.axvline(x=0.1, color='red', linestyle='--', label='Threshold (0.1)')\n",
    "    #plt.axvline(x=-0.1, color='red', linestyle='--')\n",
    "    #plt.legend()\n",
    "    #plt.gca().invert_yaxis()  # Invert y-axis to have the first covariate at the top\n",
    "    #plt.show()\n",
    "    \n",
    "    if any(abs(country_smd[smd_columns].values.flatten()) > 0.2):\n",
    "        return country\n",
    "    return None\n",
    "\n",
    "countries_to_remove = []\n",
    "countries = smd_df['corigin_iso'].unique()\n",
    "for country in countries:\n",
    "    country_with_imbalance = create_love_plot(smd_df, columns_to_check, country)\n",
    "    if country_with_imbalance:\n",
    "        countries_to_remove.append(country_with_imbalance)\n",
    "\n",
    "smd_df_filtered = smd_df[~smd_df['corigin_iso'].isin(countries_to_remove)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9ce2e7-0c6e-4522-89b0-6a72c2581ead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Use t-test to determine, if difference in life satisfaction is significant\n",
    "results = []\n",
    "\n",
    "def weighted_mean(group, value_col, weight_col):\n",
    "    return (group[value_col] * group[weight_col]).sum() / group[weight_col].sum()\n",
    "\n",
    "avg_satisfaction = matched_df.groupby(['corigin_iso', 'soep']).apply(weighted_mean, 'satisfaction', 'cem_weights').reset_index(name='weighted_satisfaction')\n",
    "\n",
    "results_dict = {}\n",
    "\n",
    "for country in avg_satisfaction['corigin_iso'].unique():\n",
    "    country_data = matched_df[matched_df['corigin_iso'] == country]\n",
    "\n",
    "    control_group = country_data[country_data['soep'] == 0]['satisfaction']\n",
    "    treatment_group = country_data[country_data['soep'] == 1]['satisfaction']\n",
    "\n",
    "    t_stat, p_value = ttest_ind(control_group, treatment_group, equal_var=False)\n",
    "\n",
    "    results_dict[country] = {\n",
    "        'T-statistic': t_stat,\n",
    "        'P-value': round(p_value, 3)\n",
    "    }\n",
    "\n",
    "significant_countries = [k for k, v in results_dict.items() if v['P-value'] < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516412a-bcab-460c-83a4-9fbaf1703392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign asterisks to plot according to significance level\n",
    "\n",
    "significance_dict = {}\n",
    "for k, v in results_dict.items():\n",
    "    if v['P-value'] < 0.01:\n",
    "        significance_dict[k] = '***'\n",
    "    elif v['P-value'] < 0.05:\n",
    "        significance_dict[k] = '**'\n",
    "    elif v['P-value'] < 0.1:\n",
    "        significance_dict[k] = '*'\n",
    "    else:\n",
    "        significance_dict[k] = ''\n",
    "\n",
    "matched_df = merged_df[merged_df['cem_matched'] == 1]\n",
    "\n",
    "def weighted_mean(group, value_col, weight_col):\n",
    "    return (group[value_col] * group[weight_col]).sum() / group[weight_col].sum()\n",
    "\n",
    "avg_satisfaction = matched_df.groupby(['corigin_iso', 'soep']).apply(weighted_mean, 'satisfaction', 'cem_weights').reset_index(name='weighted_satisfaction')\n",
    "\n",
    "# Remove Germans\n",
    "plot_data = avg_satisfaction[avg_satisfaction['corigin_iso'] != 'DEU']\n",
    "\n",
    "plt.figure(figsize=(10, 14))\n",
    "\n",
    "num_countries = len(plot_data['corigin_iso'].unique())\n",
    "country_colors = cmaps.bold(np.linspace(0, 1, num_countries))\n",
    "country_palette = dict(zip(plot_data['corigin_iso'].unique(), country_colors))\n",
    "\n",
    "# Create lines connecting the dots\n",
    "for index, country in enumerate(plot_data['corigin_iso'].unique()):\n",
    "    country_data = plot_data[plot_data['corigin_iso'] == country]\n",
    "    \n",
    "\n",
    "    if len(country_data) == 2:\n",
    "        plt.plot(country_data['weighted_satisfaction'], country_data['corigin_iso'], color=country_palette[country], linestyle='-', zorder=1, linewidth=1.0)\n",
    "\n",
    "# Create scatter plots using different markers for Control and Treatment\n",
    "sns.scatterplot(data=plot_data, y='corigin_iso', x='weighted_satisfaction', hue='corigin_iso', palette=country_palette, style='soep', markers=['.', 's'], s=100, zorder=2, legend=False)\n",
    "\n",
    "countries = plot_data['corigin_iso'].unique()\n",
    "modified_countries = [country + significance_dict.get(country, '') for country in countries]\n",
    "\n",
    "plt.yticks(ticks=np.arange(len(countries)), labels=modified_countries, rotation=0, fontsize=10)  # Use modified country labels\n",
    "\n",
    "plt.xticks()  \n",
    "plt.ylabel('', fontsize=12) \n",
    "plt.xlabel('') \n",
    "plt.ylim(len(countries) - 0.5, -0.5)\n",
    "plt.grid(color='k', linestyle='--', linewidth=0.8) \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"matching.pdf\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58af9970-5001-451a-b822-115efd1273c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "control_satisfaction = plot_data[plot_data['soep'] == 0].set_index('corigin_iso')['weighted_satisfaction']\n",
    "treatment_satisfaction = plot_data[plot_data['soep'] == 1].set_index('corigin_iso')['weighted_satisfaction']\n",
    "difference_satisfaction = treatment_satisfaction - control_satisfaction\n",
    "\n",
    "# Combine the differences with the significance levels\n",
    "difference_df = pd.DataFrame(difference_satisfaction).rename(columns={'weighted_satisfaction': 'difference'})\n",
    "difference_df['significance'] = difference_df.index.map(significance_dict)\n",
    "\n",
    "# Sort by significance first and then by the absolute difference\n",
    "difference_df['abs_difference'] = difference_df['difference'].abs()\n",
    "difference_df.sort_values(by=['significance', 'abs_difference'], ascending=[False, False], inplace=True)\n",
    "difference_df.drop('abs_difference', axis=1, inplace=True)\n",
    "\n",
    "print(difference_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22d104-a4f0-4965-8725-8209576fae18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display percentage of missing values for a selection of variables\n",
    "\n",
    "variables = ['satisfaction', 'gen_wpartner', 'gen_seppart', 'gen_wid_div', 'bad_health', \n",
    "             'continent_Asia', 'continent_North_America', 'continent_South_America', 'continent_Oceania', 'discrimination', \n",
    "             'social', 'feel_german', 'gen_vocation', 'gen_retired', 'gen_regunempl',\n",
    "             'rel_christ', 'rel_musl', 'edu_years', 'refugee', 'german', 'lang_profic', \n",
    "             \"xenophobia\", \"syear\", \"soep\", \"cohort_18_29\", \"cohort_45_59\", \"cohort_60\", \"gen_income\", \"sex\", \"corigin_iso\"]\n",
    "\n",
    "\n",
    "# Filter the data for soep = 1\n",
    "\n",
    "data_soep_1 = matched_df[(matched_df['soep'] == 1)][variables]\n",
    "\n",
    "# Calculate missing data percentage\n",
    "\n",
    "missing_data_percent = data_soep_1.groupby('syear').apply(lambda x: x.isnull().mean() * 100).drop(columns='syear')\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "sns.heatmap(missing_data_percent, cmap='viridis_r', annot=True, fmt='.2f', \n",
    "\n",
    "            cbar_kws={'label': 'Missing Data Percentage'}, annot_kws={\"size\": 8})\n",
    "\n",
    "plt.title('Percentage of Missing Data by Year and Variable (SOEP = 1, corigin_iso != DEU)', fontsize=14)\n",
    "\n",
    "plt.xlabel('Variable', fontsize=12)\n",
    "\n",
    "plt.ylabel('Year', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c20630b-0902-4746-9993-46603c85ff8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Save the original index\n",
    "original_index = data_soep_1.index\n",
    "\n",
    "# Store the 'corigin_iso' column\n",
    "corigin_iso_column = matched_df.loc[original_index, 'corigin_iso'].copy()\n",
    "\n",
    "# Exclude corigin_iso from data_soep_1\n",
    "data_soep_1 = data_soep_1.drop(columns=['corigin_iso'])\n",
    "\n",
    "# Impute missing values in data_soep_1\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "imputed_array = imputer.fit_transform(data_soep_1)\n",
    "\n",
    "# Convert the output into DataFrame and assign column names\n",
    "data_soep_1_imputed = pd.DataFrame(imputed_array, columns=data_soep_1.columns, index=original_index)\n",
    "\n",
    "# Update original matched_df with the imputed values\n",
    "for col in data_soep_1.columns:\n",
    "    matched_df.loc[original_index, col] = data_soep_1_imputed.loc[:, col]\n",
    "\n",
    "# add the corigin_iso to the imputed DataFrame\n",
    "matched_df.loc[original_index, 'corigin_iso'] = corigin_iso_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3095904-8ec9-4b82-95cd-d1b1a32a747f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display percentage of missing values for a selection of variables\n",
    "\n",
    "variables = ['satisfaction', 'gen_wpartner', 'gen_seppart', 'gen_wid_div', 'bad_health', \n",
    "             'continent_Asia', 'continent_North_America', 'continent_South_America', 'continent_Oceania', 'discrimination', \n",
    "             'social', 'feel_german', 'gen_vocation', 'gen_retired', 'gen_regunempl',\n",
    "             'rel_christ', 'rel_musl', 'edu_years', 'refugee', 'german', 'lang_profic', \n",
    "             \"xenophobia\", \"syear\", \"soep\", \"cohort_18_29\", \"cohort_45_59\", \"cohort_60\", \"gen_income\", \"refugee\", \"sex\"]\n",
    "\n",
    "\n",
    "# Filter the data for soep = 1\n",
    "\n",
    "data_soep_1 = matched_df[(matched_df['soep'] == 1)][variables]\n",
    "\n",
    "# Calculate missing data percentage\n",
    "\n",
    "missing_data_percent = data_soep_1.groupby('syear').apply(lambda x: x.isnull().mean() * 100).drop(columns='syear')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "sns.heatmap(missing_data_percent, cmap='viridis_r', annot=True, fmt='.2f', \n",
    "\n",
    "            cbar_kws={'label': 'Missing Data Percentage'}, annot_kws={\"size\": 8})\n",
    "\n",
    "plt.title('Percentage of Missing Data by Year and Variable (SOEP = 1, corigin_iso != DEU)', fontsize=14)\n",
    "\n",
    "plt.xlabel('Variable', fontsize=12)\n",
    "\n",
    "plt.ylabel('Year', fontsize=12)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed809995-cf78-481a-ab8f-0a203cdc70e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%stata -d matched_df -force\n",
    "\n",
    "destring sex, replace\n",
    "\n",
    "* Save a copy of the original dataset\n",
    "save \"original_dataset.dta\", replace\n",
    "\n",
    "* Run the regression model\n",
    "regress satisfaction sex gen_wpartner gen_seppart gen_wid_div bad_health discrimination social feel_german gen_vocation gen_retired gen_regunempl rel_christ rel_musl edu_years refugee german syear cohort_18_29 cohort_45_59 cohort_60 gen_income, robust\n",
    "\n",
    "* Save residuals in a new column\n",
    "predict satisfaction_residual, residuals\n",
    "\n",
    "* Save this modified dataset with residuals\n",
    "save \"modified_dataset.dta\", replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e487ab-0e5b-4608-9d90-46580b20acd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reimport dataframe from Stata\n",
    "merged_df = pd.read_stata(\"modified_dataset.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25d328-a127-4bb7-9f39-f305e0feedd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WIF\n",
    "freedom = pd.read_csv(\"freedom.csv\").rename(columns={'iso_alpha3': 'corigin_iso', 'year': 'syear'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87854842-23e7-4607-a87b-fd6f757aecfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Hofstede\n",
    "six_dim = pd.read_csv(\"6dimensions.csv\", sep=\";\").rename(columns={\"ctr\": \"corigin_iso\"}).drop(\"country\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b462cf7-92b4-4724-a36d-f078ccc71bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Some of this code is not redundant and not used for the thesis\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Function to assign asterisks based on p-value\n",
    "def assign_asterisks(p_value):\n",
    "    if p_value < 0.01: return '***'\n",
    "    elif p_value < 0.05: return '**'\n",
    "    elif p_value < 0.1: return '*'\n",
    "    else: return ''\n",
    "\n",
    "# Function to calculate weighted mean\n",
    "def weighted_mean(group, value_col, weight_col):\n",
    "    return (group[value_col] * group[weight_col]).sum() / group[weight_col].sum()\n",
    "\n",
    "# Calculate the weighted means and differences\n",
    "country_means = merged_df.groupby(['corigin_iso', 'soep']).apply(\n",
    "    lambda group: pd.Series({\n",
    "        'weighted_res_satisfaction': weighted_mean(group, 'satisfaction_residual', 'cem_weights'),\n",
    "        'satisfaction': weighted_mean(group, 'satisfaction', 'cem_weights')\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "country_diffs = country_means.pivot(index='corigin_iso', columns='soep', values='satisfaction').diff(axis=1)[1]\n",
    "\n",
    "# Merge the DataFrames to have one row per country\n",
    "country_data = country_means[country_means['soep'] == 1].set_index('corigin_iso')\n",
    "country_data['satisfaction_diff'] = country_diffs\n",
    "\n",
    "# Assign the German satisfaction\n",
    "deu_satisfaction = country_data.loc['DEU', 'weighted_res_satisfaction']\n",
    "\n",
    "# Assign quadrants\n",
    "conditions = [\n",
    "    (country_data['weighted_res_satisfaction'] > deu_satisfaction) & (country_data['satisfaction_diff'] > 0),\n",
    "    (country_data['weighted_res_satisfaction'] <= deu_satisfaction) & (country_data['satisfaction_diff'] > 0),\n",
    "    (country_data['weighted_res_satisfaction'] <= deu_satisfaction) & (country_data['satisfaction_diff'] <= 0),\n",
    "    (country_data['weighted_res_satisfaction'] > deu_satisfaction) & (country_data['satisfaction_diff'] <= 0)\n",
    "]\n",
    "\n",
    "values = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "country_data['quadrant'] = np.select(conditions, values)\n",
    "\n",
    "# Define color map\n",
    "colors = list(cmaps.bold.colors) \n",
    "\n",
    "# Remove DEU from the plot\n",
    "country_data_plot = country_data[country_data.index != 'DEU']\n",
    "\n",
    "# Create the scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = sns.scatterplot(data=country_data_plot.reset_index(), x='weighted_res_satisfaction', y='satisfaction_diff', hue='quadrant', palette=colors, s=100, hue_order=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "leg = scatter.legend_\n",
    "leg.set_title(None)\n",
    "for text in leg.get_texts():\n",
    "    text.set_fontsize('12')  # Set the fontsize here\n",
    "\n",
    "texts = [plt.text(row['weighted_res_satisfaction'], row['satisfaction_diff'], row['corigin_iso'] + assign_asterisks(results_dict.get(row['corigin_iso'], {}).get('P-value', 1)), fontsize=8) for _, row in country_data_plot.reset_index().iterrows()]\n",
    "adjust_text(texts)\n",
    "\n",
    "plt.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "plt.axvline(deu_satisfaction, color='gray', linestyle='--', linewidth=0.8)\n",
    "\n",
    "plt.xlabel('Residualized Satisfaction (Reference Line: Natives)', fontsize=12)\n",
    "plt.ylabel('Satisfaction Difference to Peers', fontsize=12)\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"scatter_plot.pdf\")\n",
    "plt.show();\n",
    "\n",
    "\n",
    "\n",
    "# Merge FIW and HDR data into working dataset\n",
    "country_data_merged = merged_df.merge(freedom, on=['corigin_iso', 'syear'], how='left')\n",
    "country_data_merged = country_data_merged.merge(six_dim, on=['corigin_iso'], how='left')\n",
    "\n",
    "# Merging the quadrant column into country_data_merged\n",
    "country_data_merged = country_data_merged.merge(\n",
    "    country_data[['quadrant', 'satisfaction_diff']], \n",
    "    left_on='corigin_iso', \n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Define the indicators\n",
    "indicators = ['PR_FIW', 'CL_FIW', \"pdi\", \"idv\", \"mas\", \"uai\", \"ltowvs\", \"ivr\"]\n",
    "\n",
    "# Function to calculate the mean for each indicator and quadrant\n",
    "def calculate_indicator_means(df):\n",
    "    means = {}\n",
    "    for indicator in indicators:\n",
    "        # Attempt to convert values to numeric, non-numeric become NaN\n",
    "        numeric_values = pd.to_numeric(df[indicator], errors='coerce')\n",
    "        means[indicator] = numeric_values.mean()  # NaN values are excluded from the mean calculation\n",
    "    return pd.Series(means)\n",
    "\n",
    "\n",
    "# Applying the function to each quadrant\n",
    "indicator_means_by_quadrant = country_data_merged.groupby('quadrant').apply(calculate_indicator_means).reset_index()\n",
    "\n",
    "# Transposing the DataFrame\n",
    "indicator_means_by_quadrant_T = indicator_means_by_quadrant.transpose()\n",
    "\n",
    "# Renaming the columns\n",
    "labels = {\n",
    "    'quadrant': 'Quadrant',\n",
    "    'PR_FIW': 'Political Rights',\n",
    "    'CL_FIW': 'Civil Liberties',\n",
    "    'pdi': 'Power Distance',\n",
    "    'idv': 'Individualism',\n",
    "    'mas': 'Masculinity',\n",
    "    'uai': 'Uncertainty Avoidance',\n",
    "    'ltowvs': 'Long Term Orientation',\n",
    "    'ivr': 'Indulgence'\n",
    "}\n",
    "\n",
    "indicator_means_by_quadrant_T = indicator_means_by_quadrant_T.rename(labels)\n",
    "\n",
    "# Display the transposed and renamed DataFrame\n",
    "indicator_means_by_quadrant_T\n",
    "\n",
    "\n",
    "# Labels for satisfaction variables\n",
    "\n",
    "satisfaction_variable_labels = {\n",
    "\n",
    "    'satisfaction': 'Residualized Satisfaction',\n",
    "\n",
    "    'satisfaction_diff': 'Difference to Peers',\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "# Define the satisfaction variables and labels\n",
    "satisfaction_variables = ['satisfaction', 'satisfaction_diff']\n",
    "\n",
    "# Convert indicator columns to numeric, coercing errors to NaN\n",
    "country_data_merged['PR_FIW'] = pd.to_numeric(country_data_merged['PR_FIW'], errors='coerce')\n",
    "country_data_merged['CL_FIW'] = pd.to_numeric(country_data_merged['CL_FIW'], errors='coerce')\n",
    "country_data_merged['pdi'] = pd.to_numeric(country_data_merged['pdi'], errors='coerce')\n",
    "country_data_merged['idv'] = pd.to_numeric(country_data_merged['idv'], errors='coerce')\n",
    "country_data_merged['mas'] = pd.to_numeric(country_data_merged['mas'], errors='coerce')\n",
    "country_data_merged['uai'] = pd.to_numeric(country_data_merged['uai'], errors='coerce')\n",
    "country_data_merged['ltowvs'] = pd.to_numeric(country_data_merged['ltowvs'], errors='coerce')\n",
    "country_data_merged['ivr'] = pd.to_numeric(country_data_merged['ivr'], errors='coerce')\n",
    "\n",
    "country_data_merged['PR_FIW'] = 7 - country_data_merged['PR_FIW']\n",
    "country_data_merged['CL_FIW'] = 7- country_data_merged['CL_FIW']\n",
    "\n",
    "# Update indicators\n",
    "indicators_df = country_data_merged[indicators]\n",
    "\n",
    "# Define the ranges for years in Germany\n",
    "year_ranges = [(0,100)]\n",
    "\n",
    "# Function to create and display a heatmap\n",
    "def create_heatmap(data, year_range, title_suffix):\n",
    "    filtered_data = data[data['y_in_germany'].between(*year_range)]\n",
    "    correlation_matrix = filtered_data.corr(method='kendall').loc[satisfaction_variables, indicators].T\n",
    "\n",
    "    plt.figure(figsize=(11, 8));\n",
    "    heatmap = sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=cmaps.cet_d_tritanopic_cwr, cbar=False, annot_kws={\"size\": 12})  \n",
    "\n",
    "    heatmap.set_xticklabels([satisfaction_variable_labels.get(x, x) for x in correlation_matrix.columns], rotation=0, fontsize=12)  \n",
    "    heatmap.set_yticklabels([labels.get(y, y) for y in correlation_matrix.index], rotation=0, fontsize=12)\n",
    "\n",
    "    heatmap.xaxis.tick_top()  \n",
    "    heatmap.xaxis.set_label_position('top') \n",
    "\n",
    "    title = \"\"\n",
    "    heatmap.set_title(title, fontsize=11)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"correlation.pdf\")\n",
    "    plt.show();\n",
    "\n",
    "# Loop over each year range\n",
    "for year_range in year_ranges:\n",
    "    create_heatmap(country_data_merged, year_range, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719611d0-1526-4d26-8b53-578dfcdfcfa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa37be8-5535-4c8c-bd0d-3e41d27c73d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2945776-13f9-4406-a974-a93dc583dd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
